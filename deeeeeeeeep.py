# -*- coding: utf-8 -*-
"""deeeeeeeeep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13eWr9AnkoarqkZ8-ePxbh1lUG62sfdz8
"""

!pip install -Uqq fastbook
import fastbook
# fastbook.setup_book() # mount GDrive 
from fastbook import *
from matplotlib import pyplot as plt

"""We will work on a small subset of MNIST dataset which contains only two digits (3 and 7):"""

path = untar_data(URLs.MNIST_SAMPLE)
path

path.ls()

"""We can set the BASE_PATH to path so that we start from there and not the /root/.fastai..."""

Path.BASE_PATH = path

(path).ls()

(path /"train").ls()

threes = (path/'train/3').ls().sorted()
sevens = (path/'train/7').ls().sorted()

sevens
im7_path = sevens[504]
im7 = Image.open(im7_path)
im7

# --- We can convert PILImageFile to a numpy array or a tensor (on GPU):
# im7_array = array(im7)
# plt.imshow(im7_array)
im7_tensor = tensor(im7)
plt.imshow(im7_tensor[4:70, 2:55])

# --- We can also use pandas DF to vizualize them in a neat way:
df = pd.DataFrame(im7_tensor[4:15, 4:22])
df.style.set_properties(**{'font-size': '6pt'}).background_gradient('BuPu')

"""We will start by making a simple baseline predictive model (pixel similarity):"""

#image to tensor
three_tensors = [tensor(Image.open(f)) for f in threes]
seven_tensors = [tensor(Image.open(f)) for f in sevens]

len(seven_tensors)
seven_tensors[1].shape
show_image(seven_tensors[99])

"""Let's stack our images into a two tensors and normalize them:"""

stacked_threes = torch.stack(three_tensors).float() / 255
stacked_sevens = torch.stack(seven_tensors).float() / 255
print(stacked_threes.shape,stacked_sevens.shape)

a_3 = stacked_threes[34]
a_7 = stacked_sevens[54]
print(a_3.shape,a_7.shape)

"""For this simple model, we will compare new images against the means of 3 and 7 tensors:

"""

mean3 = stacked_threes.mean(0)
mean7 = stacked_sevens.mean(0)

show_image(mean3), show_image(mean7);

print(mean3.shape,mean7.shape)

def img_distance(a, b):
     return (a - b).abs().mean()

print(img_distance(a_7, mean7))
print(img_distance(a_3, mean3))

(path / 'valid/7').ls()

"""Let's create a validation set and a simple MAE fucntion:

"""

# Stack tensors:
valid_3_tensors = torch.stack([tensor(Image.open(f)) for f in (path/'valid/3').ls()])
valid_7_tensors = torch.stack([tensor(Image.open(f)) for f in (path/'valid/7').ls()])

# Normalize tensors:
valid_3_tensors = valid_3_tensors.float()/255
valid_7_tensors = valid_7_tensors.float()/255

print(valid_3_tensors.shape,valid_7_tensors.shape)

#stacked_threes,stacked_sevens ra beham michasbanim va anha ra be yek tensor(1,28*28) tabdil mikonim
train_x = torch.cat([stacked_threes,stacked_sevens]).view(-1,28*28)
train_x.shape

train_y = tensor([1]*len(stacked_threes)+[0]*len(stacked_sevens))
print(train_y.shape,train_y)

train_y = tensor([1]*len(stacked_threes)+[0]*len(stacked_sevens)).unsqueeze(-1)
train_y.shape

valid_x = torch.cat([valid_3_tensors,valid_7_tensors]).view(-1,28*28)
valid_x.shape

valid_y =  tensor([1]*len(valid_3_tensors)+[0]*len(valid_7_tensors)).unsqueeze(-1)
valid_y.shape

dset = list(zip(train_x,train_y))
valid_dset = list(zip(valid_x,valid_y ))
x,y = dset[12000]
# x=tensor data & y=label motenazer 
print(x.shape)
print(y.shape)
print(y)

dl = DataLoader(dset,batch_size=256)
valid_dl = DataLoader(valid_dset,batch_size=256)

def init_params(size,std=1.0):
  return (torch.randn(size)*std).requires_grad_()
weights = init_params((28*28,1))
bias = init_params(1)

print(len(list(dl)))
print(len(list(dl)[23]))

print(list(dl)[23][0].shape)
print(list(dl)[23][1].shape)

print(list(dl)[23][0][100].shape)
print(list(dl)[23][1][100].shape)

sample_batch_pred = list(dl)[23][0] @ weights + bias
sample_batch_pred.shape

def linear1(xb):
  return(xb@weights + bias)

pred = linear1(list(dl)[23][0])
pred.shape

def mnist_loss(predictions,targets):
  predictions = predictions.sigmoid()
  predictions = (predictions > .5).float()
  return torch.where(targets == 1, 1-predictions,predictions).mean()
  '''error = []
  for i in range(len(targets)):
    if targets[i]==1:
       error.append(1-predictions[i])
    else:
      error.append(predictions[i])
  error = tensor(error).mean()'''

def linear1(xb):
  return xb@weights + bias

#1
def calc_grad(xb,yb,model):
  preds = model(xb)
  loss = mnist_loss(preds,yb)
  loss.backward()

from fastai.callback.schedule import lr_find
def train_epoch(model,lr,params):
  #2
 for xb,yb in dl:
#3
  calc_grad(xb,yb,model)
  for p in params:
    p.data -= p.grad*lr
    p.grad.zero_()

def batch_accuracy(xb_preds,yb):
  preds = xb_preds.sigmoid()
  correct = (preds > 0.5) == yb
  return correct.float().mean()

def validate_epoch(model):
  accs = [batch_accuracy(model(xb),yb) for xb ,yb in valid_dl]
  return round (torch.stack(accs).mean().item(),4)

validate_epoch(linear1)

weights = init_params((28*28,1))
bias = init_params(1)
params = weights, bias

for ep in range(10):
  train_epoch(linear1,0.5,params)
  print(validate_epoch(linear1),end=' ')



